---
title: "Problem Set 2"
author: Valerio Palagi, Lucia Pezzetti, Federico Testa
format: pdf
editor: visual
---

```{r, include=FALSE, echo = FALSE}
rm(list=ls())
knitr::opts_chunk$set(root.dir = normalizePath("~/GitHub/PS2_Multivariate"))
# useful commands:
# include=FALSE - results and code are not displayed
# echo=FALSE - code is not displayed
# result=FALSE - code is printed but results not
# error=TRUE - to continue execution despite errors in the chunk
# warning=FALSE - to hide warnings
# eval=FALSE - code is visible in the file but is not executed
```

```{r, include=FALSE, echo = FALSE}
library(plyr)
library(tidyverse)
library(MASS)
library(ggplot2)
library(ellipse)
library(corrplot)
library(MVN)
library(sp)
library(gridExtra)
library(reshape2)
library(ggrepel)
library(goft)
library(matrixcalc)
library(caret)
library(nnet)
library(klaR)
library(randomForest)
```

## Exercise 1

We consider the dataset `psych` which contains 24 cognitive tests administered to 301 students (with ages ranging from 11 to 16) in a suburb of Chicago: a group of 156 students (74 boys, 82 girls) from the Pasteur School and a group of 145 students (72 boys, 73 girls) from the Grant-White School.

```{r, echo=FALSE}
psych<-read.table("data/psych.txt",header=T)
#dim(psych)
head(psych)
with(psych,table(group))
```

```{r, include = FALSE, echo = FALSE}
var <- colnames(psych[,4:27])
meaning <- c("visual perception", "cubes", "paper form board", "flags", "general information", "paragraph comprehension", "sentence completion", "word classification", "word meaning", "addition", "code", "counting dots", "straight-curved capitals", "word recognition", "number recognition", "figure recognition", "object-number", "number-figure", "figure-word", "deduction", "numerical puzzles", "problem reasoning", "series completion", "arithmetic problems")

var.meaning <- as.data.frame(t(rbind(var, meaning)))
```

### 1. Use the Grant-White students data. Obtain the maximum likelihood solution for m = 5 and m = 6 factors and compute the proportion of total sample variance due to each factor. List the specific variances, and assess the accuracy of the approximation of the correlation matrix. Compare the results. Which choice of m do you prefer? Why?

First of all we start by filtering the data set in order to retain only those observations corresponding to students from the Grant-White School. Once we have done so, we can remove the variable `group` from the data set we are going to use in the analysis. Together with `group` we also discard the variable `Case`, `Sex` and `Age` in order to retain only the variables regarding the psychological tests. As a side-note we also observe that the total number of considered cases is 350 whilst we only have 301 observations, suggesting that some students did not attend the test or maybe responded only partially.

```{r}
Grant_White <- psych %>% dplyr::filter(group == "GRANT") %>%
dplyr::select(-c(Case, Sex, Age, group))
```

The aim of the Factor Analysis is, in essence, to describe - if possible - the covariance relationships among many variables in terms of a few underlying but unobservable random variables, called factors. Loosely speaking the idea is the following: suppose that the considered variables can be grouped according to their correlation pattern - i.e. that we can cluster the variables in such a way to have high intra-groups correlations and small inter-groups correlations. It seems reasonable to believe that each group of variables may be linked to a single underlying and unobservable factor, which ultimately is responsible for the observed significant intra-group correlations.

Looking at the `psych` data set it is conceivable that the results of the "ability" tests performed are just different measures of fewer broad domains like for example verbal or spatial abilities, memory or mathematical deduction.

We start our analysis by considering the correlation matrix. It is helpful to plot it:

```{r, echo=FALSE}
R <- cor(Grant_White)
#testRes = cor.mtest(Grant_White, conf.level = 0.95)
```


```{r, fig.width = 13, fig.height = 13, echo=FALSE}
corrplot(R, method = 'color', type = 'lower', insig='blank',
         addCoef.col ='black', number.cex = 0.8, diag=FALSE, col = COL2('RdBu', n = 200))
```

Looking at the plot of the correlations, we can immediately observe the presence of some groups of quite significantly correlated variables. The most evident one is composed by the variables `V5`, `V6`, `V7`, `V8` and `V9`. Looking at the type of the tests they represent, we can observe that all those variables are related to a linguistic or verbal area, hence we do expect them to be highly correlated, and we may also expect them to be explained by a common factor.

With a slightly greater level of uncertainty, we can also identify the group of the variables `V10`, `V11`, `V12` and `V13`. In this case the relationship among the variables is less clear, since we do not know how the tests have been performed and what they aim to measure. Nevertheless we may think to coding, additions, counting dots and recognition of capital letters as quite simple tasks and so we may expect that the students who score higher in these tests may be the most precise or focused. This may explain why the results of these tests are quite highly correlated.

Finally, we also mention the group of the variables `V20`, `V21`, `V22`, `V23` and `V9` - all quite related to some logical or problem solving skills.

Overall, we can say that almost the totality of the correlations are positive and that the majority of them are quite small - or at least smaller than 0.5.

```{r, include = FALSE, echo = FALSE}
Grant_White <- scale(Grant_White)

n <- nrow(Grant_White)
p <- ncol(Grant_White)
R <- cor(Grant_White)
```

In particular, we are asked to perform a Factor Analysis with $m=5$ and $m=6$ common factors using the maximum likelihood approach. In R we can use the command `factanal`, however in applying this function we rely on the strong assumption of normality of the data. Therefore we start by assessing this assumption.

To do so we plot the chi-square qq-plot of the squared Mahalanobis distances: under the assumption of multivariate normality, the data will fall along the bisector.

```{r, echo=FALSE}
x.bar <- colMeans(Grant_White)
S <- cov(Grant_White)

d = mahalanobis(Grant_White, center = x.bar, cov = S);

ggplot(as.data.frame(d), aes(x = qchisq(ppoints(d), df = p), y = sort(d))) + 
 geom_point() + geom_abline(intercept = 0, slope = 1, col = "red", lwd = 1) +
  expand_limits(x = c(9.0, 60.0), y = c(9.0, 60.0)) +
  ggtitle("Chi-squared QQ-plot of squared generalized distances\n Grant-White school") +
  theme(plot.title = element_text(hjust = 0.5))
```

As we can see from the plot, the points deviate from the straight line only in the right top end. This indicates that the largest values are more spread out than for a $\chi_{24}^2$: the tail of the chi-square distribution is lighter. So, the plot seems to suggest that the Mahalanobis distances in the sample are more right skewed than you would expect to see with a multivariate normal.

Nevertheless, we also observe that the points responsible for this potential non-gaussian behavior are only three, hence they could simply be multivariate outliers.

```{r, echo = FALSE}
a <- (n-0.5)/n
x = 1:length(d)
outliers <- which(d > qchisq(a, df = p))
labels <- rep("", n)
labels[outliers] <- as.character(psych$Case[nrow(psych[psych$group == "PASTEUR",]) + outliers])
colors <- rep("black", n)
colors[outliers] <- "red"

ggplot(as.data.frame(d), aes(x = x, y = d, label = labels)) + 
  geom_point(pch = 16, col = colors) + 
  geom_text(hjust = 0.2, nudge_x = -0.05, nudge_y = 2.0, size = 4) +
  geom_hline(yintercept = qchisq(a, df = p), col = "red", lty = 2) + 
  ggtitle("Multivariate outliers") + 
  ylab("squared Mahalonobis distances") + 
  xlab("indexes") + 
  theme(plot.title = element_text(hjust = 0.5))
```

As we can immediately see from the plot, observations having values 213 and 225 of the `Case` variable - in the original data set - are multivariate outliers: it is then customary to temporarily remove them and study again the multivariate normality of the dataset.

```{r, echo=FALSE}
without_outliers <- Grant_White[-c(12,24),]
x.bar <- colMeans(without_outliers)
S <- cov(without_outliers)

d = mahalanobis(without_outliers, center = x.bar, cov = S);

ggplot(as.data.frame(d), aes(x = qchisq(ppoints(d), df = p), y = sort(d))) + 
 geom_point() + geom_abline(intercept = 0, slope = 1, col = "red", lwd = 1) +
  expand_limits(x = c(9.0, 60.0), y = c(9.0, 60.0)) +
  ggtitle("Chi-squared QQ-plot of squared generalized distances\n without outliers") +
  theme(plot.title = element_text(hjust = 0.5))
```

The chi-square qq-plot of the Mahalanobis distances now seems to fit well the assumption of multivariate normality.

As a final further test, we list the result of the Henze-Zirkler's multivariate normality test and of the Mardia's multivariate skewness and kurtosis coefficients as well as their corresponding statistical significance. The tests are performed using the function `mvn` in the `MVN` package. We test the whole data set:

```{r, echo = FALSE}
#mvShapiro.Test(as.matrix(scale(Grant_White)))
multnorm_hz <- mvn(Grant_White, scale = T)
multnorm_hz$multivariateNormality
multnorm_mardia <- mvn(Grant_White, mvnTest = "mardia", scale = T)
multnorm_mardia$multivariateNormality
```

as already highlighted using the chi-square qq-plot, the problem seems to lie in the skewness of the data. If then we re-do the analysis without taking into account the outliers:

```{r, echo = FALSE}
multnorm_hz <- mvn(without_outliers, scale = T)
multnorm_hz$multivariateNormality
multnorm_mardia <- mvn(without_outliers, mvnTest = "mardia", scale = T)
multnorm_mardia$multivariateNormality
```

All problems seem to disappear and multivariate normality can be taken for granted.

We can now legitimately proceed with the maximum likelihood approach to perform the Factor Analysis. As requested, we consider and compare the choices of $m=5$ and $m=6$ factors without applying, for now, any rotation (that we will use later for interpretation purposes).

```{r}
m = c(5,6)
psych.5fa.ml <- factanal(x=Grant_White, factors = m[1], rotation = "none")
psych.6fa.ml <- factanal(x=Grant_White, factors = m[2], rotation = "none")
```

We start by looking at the obtained loadings, in the case of 5 factors:

```{r}
L.5.ml <- psych.5fa.ml$loadings; L.5.ml
```

and of 6:

```{r}
L.6.ml <- psych.6fa.ml$loadings; L.6.ml
```

In particular the proportion of variance explained by each factor is computed as the sum of the squared loadings of the considered factor divided by the number of variables $p=24$. We get the following results:

```{r}
L5 <- psych.5fa.ml$loadings
L6 <- psych.6fa.ml$loadings

sL5 <- colSums(L5^2)
sL6 <- colSums(L6^2)

rbind(`Proportion Var` = round(sL5/p, 3L))
rbind(`Proportion Var` = round(sL6/p, 3L))
```

As we have seen in the lectures, the proportion of variance explained by each factor indicates how much of the total variability of the original variables is accounted for by that particular factor. Specifically, it represents the proportion of the total variance in the observed variables that can be attributed to that factor. For this reason it is usually used to address the relevance of the factors and, consequently, as a support to the choice of the number of factors to retain.

As a general guideline we can say that factors that explain a large proportion of the variance of the original variables are considered more important and may be more useful for further analysis. Conversely, factors that explain a small proportion of the variance may be less useful and can potentially be dropped from further analysis.

Nevertheless it is important to always keep in mind that there is not a unique way to decide how many factors we should retain. Each situation need to be analyzed separately and other factors - such as interpretability and theoretical considerations - may play a relevant role.

In our situation we can observe that the first factor explains quite a relevant percentage - the 30% - of the total variability of the data. Therefore we expect it to be in some sense the most important of all factors and to represent an "important cognitive skill". The other factors account for a much smaller proportion. Surely the less explanatory ones are the fifth and the sixth - when present - factors, as they respectively explain the 2.2/2.1% and the 1.7% of the total variance. In particular, the fact that they explain a relatively small but similar proportion of variance seems to suggest that we should reserve the same treatment to both of them - i.e. retain or discard them both. However, as said before, the decision to discard a factor should not be based solely on the proportion of variance it explains, hence before making any further consideration we proceed in our analysis.

Given the proportion of variance accounted for by each factor, we can also compute the cumulative proportion of variance explained by the $m$ factors. To do so we simply apply the function `cumsum` to the single proportion of variances.

```{r}
rbind(`Cumulative Var` = round(cumsum(sL5/p), 3L))
rbind(`Cumulative Var` = round(cumsum(sL6/p), 3L))
```

As we can see, both the factor analysis performed with 5 factors and the one with 6 factors only explain slightly more than the 50% of the total variability of the data. Therefore, they don't seem to be a good fit for the data.

Now, in a real-life study, there are several things we can do to try improve the fit of the model, some examples include:

-   removing the outliers from the data set and re-estimate the maximum likelihood solution;
-   using different methods to perform the factor analysis - for example a possibility is to use the principal component approach;
-   finally, we may consider alternative analyses - such as principal component analysis, cluster analysis, or discriminant analysis.

We will not try any of these improvement, but - for the sake of the exercise - we will proceed in the factor analysis taking this issue into account.

Other relevant features to examine are the specific variances, or uniquenesses. The $i^{th}$ uniqueness represents the proportion of variance of the $i^{th}$ variable that is not accounted for by the factors. In other words, the specific variances are important because they provide information about the unique contribution of each observed variable to the total variance. Variables with large specific variances are those that are not well accounted for by the underlying factors. These variables are also more likely to have weaker factor loadings and may be more difficult to interpret. On the other hand, variables with small specific variances are those that are well explained by the factors and are more likely to have stronger factor loadings.

For all these reasons, the specific variances can also affect the decision of how many factors to retain in the analysis: if there are many observed variables with high specific variances, this may indicate the necessity to add some extra factors.

The specific variances can also be used to assess the overall fit of the factor model. If the specific variances are very large, this may indicate that the model is not a good fit for the data and may need to be modified Alternatively, if the specific variances are very small, this may indicate that the model is over-fitting the data and may need to be simplified.

Overall, specific variances are an important part of the output of factor analysis and can provide valuable information about the quality of the factor model and the interpretation of the results.

We list our results:

```{r, echo = FALSE}
uniq <- cbind(psych.5fa.ml$uniquenesses, psych.6fa.ml$uniquenesses)

uniq <- as.data.frame(round(uniq, 3))
colnames(uniq) <- c("uniquenesses_nfactors_5", "uniquenesses_nfactors_6")
uniq
```

As we could expect from the fact that the total variance explained by the common factors is quite low, the uniquenesses are generally quite high. Moreover, there are no variables with a specific variance that can be considered particularly small: the variable that is best explained by the factors has a uniqueness greater than 0.2.

We are mainly interested in two different aspects:

-   high specific variances (or very low ones, but there isn't any in our case);
-   significant difference among the uniquenesses returned using 5 or 6 factors.

For what concern the higher specific variances, we can observe that there are 16 variables for which the uniquenesses in both cases are - for example - higher than 0.4, specifically:

```{r, echo = FALSE}
uniq %>% dplyr::filter(pmin(uniquenesses_nfactors_5, uniquenesses_nfactors_6) > 0.4)
```

Again, the presence of such a great number of variables with high uniquenesses may suggests that the model does not fit the data very well.

Keeping this in mind, it is also important to note that high uniqueness values are not necessarily a problem in themselves. Some variables will naturally have higher uniqueness values than others, depending on their level of specificity or measurement precision. Therefore one should always consider the specific context of the phenomenon being studied.

In our analysis, the presence of such a high number of variables with high uniqueness value may not be completely unexpected: the data are related to aptitude tests on students, thus it seems reasonable to expect that the tests also rely heavily on individual abilities of pupils that the common factor would fail to encode.

Nevertheless, the number of factors with a great amount of unexplained variability seems to us too high to be ignored, even if we keep into account the individual characteristics of the students.

Let's now look at the variables for which the inclusion of the sixth factors plays a relevant role.

In particular we decided to print the specific variances of the variables for which the addition of the sixth factor lead to a decrease - or increase - of the specific variance greater than 0.1. We also return them in decreasing order: the first variable returned is the one with the greatest gap among the two uniquenesses.

```{r, echo = FALSE}
uniq %>% dplyr::filter(abs(uniquenesses_nfactors_5 - uniquenesses_nfactors_6) > 0.1) %>% dplyr::arrange(desc(uniquenesses_nfactors_5 - uniquenesses_nfactors_6))

```

As we can observe, the variables that are mainly affected by the introduction of a new factor are `V17` - an object-number test - and `V11` - a code-test. So we expect these two variables to be better explained by 6 common factors than 5.

It is also worth noticing that the number of variables for which the uniquenesses are significantly reduced by the introduction of the sixth factor is only two. This may suggest that the model with 6 factors may fail in providing a relevant improvement with respect to the one with 5 factors.

We can now assess the approximation of the correlation matrix. To do so we can compute the residual matrix 
$$
\begin{aligned}
  \mathbf{R} - \hat{\mathbf{L}}\hat{\mathbf{L}}^T - \hat{\mathbf{\Psi}}
\end{aligned}
$$

resulting from the approximation of $\mathbf{R}$ via the simpler structure $\hat{\mathbf{L}}\hat{\mathbf{L}}^T + \hat{\mathbf{\Psi}}$. We can then summarize how far from the perfect approximation we are by computing its Frobenius norm.

```{r}
Psi.5.ml <- diag(psych.5fa.ml$uniquenesses, p)
Psi.6.ml <- diag(psych.6fa.ml$uniquenesses, p)

Residual.5.ml <- R - (L.5.ml%*%t(L.5.ml) + Psi.5.ml)
Residual.6.ml <- R - (L.6.ml%*%t(L.6.ml) + Psi.6.ml)

Frob.res <- cbind(sum(Residual.5.ml^2), sum(Residual.6.ml^2))

row.names(Frob.res) <- "Frobenius norm of the residual matrix: "
colnames(Frob.res) <- c("nFactors_5", "nFactors_6")

as.data.frame(round(Frob.res, 3))
```

The obtained norms - for the choice of 5 and 6 factors - are in both cases quite high, even if adding the sixth factor slightly reduces it. This may again be explained by keeping into account the fact that in both cases the total cumulative variance explained by the factors is quite small - 0.503 and 0.525 respectively. Therefore, we can conclude that, despite the improvement in the approximation related to the inclusion of the sixth factor, in both cases the approximation of the correlation matrix is not so satisfactory.

Overall, the comparison seems to suggests that neither the 5 factors model nor the 6 factors one are good fit for the data. Anyway, if we stick to the problem of deciding how many factors to retain, the choice of 5 common factors may appear to be the preferable one. Indeed, as it emerges from all the comments above, the improvements related to the use of six factors are not sufficiently noteworthy.

### 2. Give an interpretation to the common factors in the $m = 5$ solution with varimax rotation

To give an easier interpretation of the factors, we can perform a varimax rotation. The varimax technique seeks the rotated loadings $\mathbf{L}^*$ that maximize the variance of the squared loadings for each factor. Loosely speaking, this maximization has the effect of "spreading out" the squares of the loadings on each factor as much as possible. In other words, the varimax rotation technique attempts to make the loadings either small or large to facilitate interpretation. Therefore performing a varimax rotation aims to find groups of large and the negligible in each column of the rotated loadings matrix $\mathbf{L}^*$.

```{r}
psych.5fa.ml <- factanal(x=Grant_White, factors = m[1], rotation = "varimax")
psych.5fa.ml$loadings
```

Before proceeding with the interpretation it is worth noticing that the fifth factor explains a proportion of variance that is much smaller than the other four. Thus we expect it to be the most difficult to interpret.

In order to interpret the factors we classify the observed variables into five groups - each one corresponding to a different factor - based on their loadings.

As a first subdivision we simply choose to assign each variable to the factor corresponding to its greatest loading. Then we assess the significance of the loadings using a threshold value equal to 0.5.

```{r, echo=FALSE}
maximum <- apply(psych.5fa.ml$loadings, 1, max)
pos.max <- as.data.frame(which(psych.5fa.ml$loadings == maximum, arr.ind = T))
pos.max <- pos.max %>% dplyr::mutate(load.max = round(psych.5fa.ml$loadings[as.matrix(pos.max)], 3))

group1 <- row.names(dplyr::filter(as.data.frame(pos.max), col == 1))

group2 <- row.names(dplyr::filter(as.data.frame(pos.max), col == 2))

group3 <- row.names(dplyr::filter(as.data.frame(pos.max), col == 3))

group4 <- row.names(dplyr::filter(as.data.frame(pos.max), col == 4))

group5 <- row.names(dplyr::filter(as.data.frame(pos.max), col == 5))
```

The group of variables associated to the first factor is given by:

```{r, echo=FALSE}
group1 <- var.meaning %>% dplyr::filter(var %in% group1) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max), col == 1)$load.max)

group1
```

We can immediately notice that the considered loadings of all the variables in this group have a high enough level of significance.

The interpretation of this factor seems also quite straightforward, as all the variables associated to it are related to some kind of linguistic and comprehension abilities. The tests indeed are devised to target students capacity to understand the meaning of a phrase or a word as well as their aptitude to correctly identify the class of a word or complete a sentence.

For this reason we may denote it as "verbal intelligence".

The group of variables associated to the second factor is:

```{r, echo=FALSE}
group2 <- var.meaning %>% dplyr::filter(var %in% group2) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max), col == 2)$load.max)

group2
```

Based on our significance threshold value, only the following four variables should be retained:

```{r, echo=FALSE}
restricted2 <- group2 %>% dplyr::filter(load.max > 0.5)
restricted2
```

The variables `V1`-`V3`-`V4` seem to be related to the ability to deal with spatial and visual relations or, more precisely, to the ability of an individual to visualize and manipulate objects in space. These capacities surely play also a relevant role in determining one's skill to complete a given series. However, the results of the $23^{th}$ test is necessarily also connected to some problem-solving and logical capabilities, as it requires some imagery capacity, mental visualization skills and part-whole relationship skills.

If we also look at the excluded variables - `V2`, `V20`, `V22` - all seems to be in some sense related to the already identified visual, spatial and logical spheres. This give further support to our interpretation.

Consequently, we can call this factor "visual,spatial and logic intelligence".

The group of variables associated to the third factor is:

```{r, echo=FALSE}
group3 <- var.meaning %>% dplyr::filter(var %in% group3) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max), col == 3)$load.max)

group3
```

Even for this factor there is one variable that we do not consider significant, `V21` - actually, we have rounded up the loading of `V24`. So the variables to be considered are:

```{r, echo=FALSE}
restricted3 <- group3 %>% dplyr::filter(load.max > 0.49)
restricted3
```

Looking at this group - and also taking into account the fact that the variable `V10` represents an addition test and loads highly on the considered third factor - we may interpret this factor as an "arithmetic intelligence".

For what concerns the excluded variable `V21`, it corresponds to a numerical-puzzle test and therefore fits quite well in the group associated with an "arithmetic intelligence" factor.

The group of variables associated to the fourth factor is:

```{r, echo=FALSE}
group4 <- var.meaning %>% dplyr::filter(var %in% group4) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max), col == 4)$load.max)
group4
```

We remove the three variables whose loadings are not significant enough:

```{r, echo=FALSE}
restricted4 <- group4 %>% dplyr::filter(load.max > 0.5)
restricted4
```

All the variables that load high on the fourth factor are related to the spheres of recognition and association. Thus this factor may be simply called "recognition".

Once again the excluded variables may be well explained by the "recognition" fourth rotated factor.

Finally, the group of variables associated to the fifth factor is:

```{r, echo=FALSE}
var.meaning %>% dplyr::filter(var %in% group5) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max), col == 5)$load.max)
```

This group is made of a unique variable, `V13`, corresponding to a test involving straight and curved uppercase letters. The factor may be called "letter recognition", but is actually of low relevance.

We can now deal with the variables with an "ambiguous" classification.

In particular, the variables for which we have identified some issues are the following:

```{r, echo=FALSE}
doubt.var <- rbind(group2 %>% dplyr::filter(load.max < 0.5), group3 %>% dplyr::filter(load.max < 0.5), group4 %>% dplyr::filter(load.max < 0.5))
doubt.var <- round(psych.5fa.ml$loadings[which(var.meaning$var%in%doubt.var$var),],3)
doubt.var <- cbind(doubt.var, dplyr::filter(var.meaning, var %in% row.names(doubt.var)))
doubt.var <- dplyr::select(doubt.var, -var)
doubt.var
```

As we can see, `V2` may properly be associated with the second group, indeed its second factor loading is much greater than all the others. For the same reason the variables `V18`, `V19`, `V20` and `V24` may be reasonably added to groups 4, 4, 2 and 3 respectively. So, with the only exceptions of variables `V11` and `V22`, our previous identification of the groups seems to be quite satisfactory, especially if we consider the fact that the proportion of cumulative variance explained by the 5 considered common factors is only equal to the 50% of the total variability of the data.

For what concerns the variable `V11`, we can notice that its three highest loadings - corresponding to factors 3, 4 and 5 - are all very similar. This may be reasonable, indeed to score high in a code test it in necessary not only to have good "matching" capacities (factor 4), but also some mathematical-problem solving skills (factor 3) and the ability to recognize quickly the elements to code (factor 5).

Similarly, for variable `V22` we cannot ignore the loading associated to factor 1, besides the higher one being the one of factor 2. Also in this case, we may interpret this result by saying that "problem reasoning" abilities are quite transversal skills and are surely also related to the verbal sphere.

### 3. Make a scatterplot of the first two factor scores for the $m = 5$ solution obtained by the regression method. Is their correlation equal to zero? Should we expect so? Comment.

First of all we make some theoretical observations on the factor scores.

The factor scores are the estimated values of the underlying common factors. In particular, they are estimates of the unobserved vector $f_i = (f_{i1}, \dots, f_{im})$ - in place of which we have only observed the variables realizations $x_i = (x_{i1}, \dots, x_{im})$. The estimation, however, is not straightforward, as the total number of unobserved quantities - given not only by the $f_i$, but also by the error terms $\epsilon_i$ - outnumbers the observed $x_i$.

One of the most used approaches advanced to overcome this problem is the *regression method*. The idea is the following: consider the baseline equation of the factor model 
$$
\begin{align}
  X - \mu = \mathbf{L}F + \epsilon
  \end{align}
$$
where we suppose that both the factors and the errors are jointly normally distributed and that both the common factors and the errors are uncorrelated within each other. Under these assumption we know that the conditional distribution of $F|X$ is again Gaussian, with conditional mean given by 
$$
 \mathbf{L}^T\mathbf{\Sigma}^{-1}(X-\mu) = \mathbf{L}^T(\mathbf{LL}^T -\mathbf{\Psi})^{-1}(X-\mu)
$$
Given so, a natural estimate for $f_i$ is simply the corresponding estimate of this conditional mean: 
$$
\hat{f_i} = \hat{\mathbf{L}}^T\mathbf{S}^{-1}(x_i-\bar{x})
$$
where we use $\mathbf{S}$ in place of its estimation in order to try to reduce the effects of possible mistakes in determining the number of factors.

Given so, since we assume that the common factors are uncorrelated - $cov(F_i, F_j) = 0, \quad \forall i\neq j$ - we will get - or at least we would like to get - uncorrelated factor scores.

We therefore expect that the first and second factor scores are essentially uncorrelated.

To test our hypothesis we make a scatter plot of the first two factor scores:

```{r}
psych.5fa.ml <- factanal(x=Grant_White, factors = m[1], rotation = "varimax",
scores = "regression")
scores_GrWh <- psych.5fa.ml$scores
```

```{r, fig.height=4, fig.width=4}
ggplot(as.data.frame(scores_GrWh), aes(x = Factor1, y = Factor2)) + 
  geom_point() +
  ggtitle("Scatter-plot of the first two factor scores\n for the Grant-White students") +
  theme(plot.title = element_text(hjust = 0.5, size = 10))

```

As expected, no clear pattern arises from the scatter plot, suggesting that the first two factor scores are almost uncorrelated within each other. Moreover, a numerical computation of the correlation returns:

```{r, echo=FALSE}
data.frame("correlation" = cor(scores_GrWh[,1], scores_GrWh[,2]))
```

A value that is indeed small.

Finally, a further insightful thing to verify is whether the assumption of bivariate normality actually holds: in particular, by the theory we expect that the first and second factors are jointly normally distributed with mean zero and identity covariance matrix. This is translated in a plot in which the points are centered around the origin and the elliptic contour plots have a nice circular shape.  

Let's first of all compute the sample means of the two factor scores:

```{r, echo=FALSE}
means.hat <- colMeans(scores_GrWh[,c(1,2)])
means.hat <- data.frame("x_bar" = means.hat[1], "y_bar" = means.hat[2], row.names="Grant-White:")
means.hat
```

Before plotting the data, we also notice that together with the assumption of normality, it is important to study the presence of bivariate outliers. Indeed, outliers can influence the strength and direction of the correlation. The reason is that outliers can create a distorted picture of the relationship between the two variables. Thus, if an outlier is included in a dataset, it can pull the correlation coefficient in one direction or another, making it appear stronger or weaker than it actually is, or even falsify the normality assumption.

In our situation, the correlation coefficient is already very low, therefore we do not expect to find bivariate outliers that are extreme observations for both observations. For the same reason, we also expect to find circular contour plots. The ellipses shown in the plot are fit to the data assuming a Gaussian distribution.

```{r, fig.width=8, fig.height=8, echo = FALSE}
a_GW <- (n-0.5)/n
S <- var(scores_GrWh)
label_GW = rep("", n)

df_ell1 = data.frame(ellipse(x = S[c(1,2), c(1,2)], centre = c(means.hat$x_bar, means.hat$y), level = 0.95))

df_ell2 = data.frame(ellipse(x = S[c(1,2), c(1,2)], centre = c(means.hat$x_bar, means.hat$y), level = a_GW))

pl <- ggplot(as.data.frame(scores_GrWh), aes(x = Factor1, y = Factor2)) + 
  geom_point() + geom_path(data=df_ell2, aes(x=Factor1, y=Factor2))

# before plotting we determine the potential bivariate outliers.
# Extract components
build <- ggplot_build(pl)$data
points <- build[[1]]
ell <- build[[2]]

# Find which points are inside the ellipse, and add this to the data
dat <- data.frame(
  points[1:2], 
  in.ell = as.logical(point.in.polygon(points$x, points$y, ell$x, ell$y))
)

label_GW[!dat$in.ell] = as.character(psych$Case[nrow(psych[psych$group == "PASTEUR",]) + which(!dat$in.ell)])

pl_sc_GW <- ggplot(dat, aes(x = x, y = y)) + geom_point(aes(col = in.ell), show.legend = F) + geom_text(mapping = aes(label = label_GW),
            hjust = 1.0, nudge_y = -0.15, nudge_x = 0.05, size = 5, parse = T) +
geom_path(data=df_ell1, aes(x=Factor1, y=Factor2), color = "royalblue2", linetype = 2) + 
  geom_path(data=df_ell2, aes(x=Factor1, y=Factor2), color = "darkorange") + 
  scale_color_manual(values = c("red", "black")) + 
  geom_text(mapping = aes(x = 0.25, y = 2.7, label = "level = (n-0.5)/n = 0.99655"), 
            colour = "darkorange") +
  geom_text(mapping = aes(x = 0.25, y = 2, label = "level = 0.95"), 
            colour = "royalblue2") +
  geom_point(mapping = aes(x=means.hat$x_bar, y=means.hat$y_bar), 
             color = "darkorchid3", size = 3) + 
  geom_text(mapping = aes(x = means.hat$x_bar, y = means.hat$y_bar, 
                          label = "(x_bar, y_bar)"), nudge_x = 0.3, 
            nudge_y = -0.1, colour = "darkorchid3") +
  xlab("Factor 1") + ylab("Factor 2") + 
  ggtitle("Scatter-plot of the first two factor scores\n for the Grant-White students") +
  theme(plot.title = element_text(hjust = 0.5, size = 20)) +
  xlim(-3.3, 3.3) + ylim(-3.3, 3.3)
pl_sc_GW
```

As expected, the desired assumption of normality appears to hold. Indeed the shape of the bivariate distribution of the first two scores well resemble a standard bi-dimensional Gaussian. We also notice the presence of one bivariate outlier that has been identified by means of the variable `Case` in the original data set.


### 4. Obtain the maximum likelihood solution with varimax rotation for $m = 5$ factors by using the Pasteur students data. Is the interpretation to the common factors similar to that of Grant--White students?

As requested we now compute the maximum likelihood solution with 5 factors using the data on the students attending the Pasteur school. As before we only consider the variables related to the 24 attitudinal tests.

```{r}
Pasteur <- psych %>% dplyr::filter(group == "PASTEUR") %>%
dplyr::select(-c(Case, Sex, Age, group))
```

Let's first of all look at the correlations among those variables.

```{r, echo=FALSE}
n <- nrow(Pasteur)
p <- ncol(Pasteur)
R_past <- cor(Pasteur)
```

```{r, fig.width = 13, fig.height = 13, echo=FALSE}
corrplot(R_past, method = 'color', type = 'lower', insig='blank',
         addCoef.col ='black', number.cex = 0.8, diag=FALSE, col = COL2('RdBu', n = 200))
```

As happened for the data related to the Grant-White school, we can observe that there is a group of high correlated data corresponding to variables `V5`, `V6`, `V7`, `V8` and `V9`. This, again, may suggest the existence of an underlying factor which all those variable refer to. Those variables are also quite correlated with `V21`, `V22` and `V23`. Finally some other less clear correlation patterns appear among the variables `V10`, `V11`, `V12` and `V13` and maybe also among `V14`, `V15` and `V16`.

Overall, however, the great majority of correlation coefficients are quite small and almost all positive.

We also quickly assess the assumption of multivariate normality:

```{r, echo=FALSE}
x.bar <- colMeans(Pasteur)
S <- cov(Pasteur)

d = mahalanobis(Pasteur, center = x.bar, cov = S);

ggplot(as.data.frame(d), aes(x = qchisq(ppoints(d), df = p), y = sort(d))) + 
 geom_point() + geom_abline(intercept = 0, slope = 1, col = "red", lwd = 1) +
  expand_limits(x = c(9.0, 60.0), y = c(9.0, 60.0)) +
  ggtitle("Chi-squared QQ-plot of squared generalized distancees\n Pasteur School") + theme(plot.title = element_text(hjust = 0.5))
```

As we can see, the assumption of normality seems to perfectly fit the data coming from the Pasteur school.

We can now fit the model for the Pasteur data using the `factanal` function and 5 factors. Since we are asked to compare the interpretation of the factors with the one obtained using data of the students in the Grant-White school, we already perform the factor analysis using the varimax rotation.

```{r}
psych.5fa.ml <- factanal(x=Pasteur, factors = m[1], rotation = "varimax")
psych.5fa.ml$loadings
```

Also in this case we can observe that the proportion of cumulative variance explained by the five factors is simply equal to the 48.6%, a percentage that is not so satisfactory. We can also notice that the proportion of variance explained by the last factor is doubled with respect to the data from the Grant-White school, hence we expect a slightly easier interpretation of the fifth factor than before. Conversely the proportion of variance explained by factor 3 and 4 are smaller. This may suggest the presence of some differences between the interpretations.

We group the variable exactly as we have done for the Grant-White data:

-   firstly, for each variable we detect its greatest loading and the corresponding factor;
-   then we assess the significance of the loading by means of a threshold value of 0.5.

```{r, echo=FALSE}
maximum_past <- apply(psych.5fa.ml$loadings, 1, max)
pos.max_past <- as.data.frame(which(psych.5fa.ml$loadings == maximum_past, arr.ind = T))
pos.max_past <- pos.max_past %>% dplyr::mutate(load.max = round(psych.5fa.ml$loadings[as.matrix(pos.max_past)], 3))

group1_past <- row.names(dplyr::filter(as.data.frame(pos.max_past), col == 1))

group2_past <- row.names(dplyr::filter(as.data.frame(pos.max_past), col == 2))

group3_past <- row.names(dplyr::filter(as.data.frame(pos.max_past), col == 3))

group4_past <- row.names(dplyr::filter(as.data.frame(pos.max_past), col == 4))

group5_past <- row.names(dplyr::filter(as.data.frame(pos.max_past), col == 5))
```

The first group is made up by the variables:

```{r, echo=FALSE}
group1_past <- var.meaning %>% dplyr::filter(var %in% group1_past) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max_past), col == 1)$load.max)

group1_past
```

All the variables in this group have high loadings, moreover the variables in this group are exactly the same we have found in the group related to the first common factor of the Grant-White data. Therefore we may rightly say that, in both cases - with the Grant-White or Pasteur data - the first common factor may be interpret as "verbal intelligence".

The second group comprehends:

```{r, echo=FALSE}
group2_past <- var.meaning %>% dplyr::filter(var %in% group2_past) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max_past), col == 2)$load.max)

group2_past
```

among which the variables with a loading higher then the fixed significant threshold value are:

```{r, echo=FALSE}
restricted2_past <- group2_past %>% dplyr::filter(load.max > 0.5)
restricted2_past
```

Even for this second group we can observe a good correspondence to what we have found using the Grant-White data. Indeed, the variables that load sufficiently high on this factor are almost the same then in the previous analysis. If then we consider also the removed variables whose loadings are smaller than 0.5, we can see a perfect correspondence between the two groups. Hence we can still interpret this second factor as "visual, spatial and logic intelligence".

Let's switch to the third group:

```{r, echo=FALSE}
group3_past <- var.meaning %>% dplyr::filter(var %in% group3_past) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max_past), col == 3)$load.max)

group3_past
```

and removing the variable with less significant loadings:

```{r, echo=FALSE}
restricted3_past <- group3_past %>% dplyr::filter(load.max > 0.5)
restricted3_past
```

Comparing with the results obtained in the Grant-White data analysis, we can observe that this group appears to correspond to the group which - in that analysis - was related to the fourth factor. Despite this inversion of order we can reasonably say that the interpretation of the third factor we are considering and of the previous fourth factor are basically the same. Indeed, even in this case all the identified variables are related to the spheres of recognition.

We can also underline that the only difference among the two groups (before "cutting" at 0.5) is the absence of the variable `V11` in this latter one. This can be considered a subtle and not so relevant disagreement: `V11` indeed had already a loading (0.436) below the significance threshold value and actually showed other two loadings of a very similar magnitude (0.423 in correspondence of factor 3 and 0.418 on factor 5). For all those reasons, its classification is far more clear than in the previous analysis.

To conclude, we can assimilate this third factor to the previous fourth one: reasonably they can be both interpreted as "recognition skills".

At this point we expect to see a fourth group similar to the previous third or fifth one:

```{r, echo=FALSE}
group4_past <- var.meaning %>% dplyr::filter(var %in% group4_past) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max_past), col == 4)$load.max)

group4_past
```

The result we get however is different, let's look at it. Firstly we notice that all the variables have a loading higher then 0.5, thus all have a level of significance that is high enough. The three variables are at first glance not much related, nevertheless - as already mentioned at the very beginning, during the comment relative to the correlation plot of the Grant-White data - all these three test, in our opinion, involve quite simple tasks for children whose age ranges between 11 to 16 years old. For this reason, we believe that the results of all these tests heavily rely on students ability to stay focused for a long time and be precise, maybe even under pressure. Given this interpretation of the test, it is quite natural to denote this factor as "focus" or "precision".

Actually, taking into account the previous difficulties in determining the group for `V11` as well as the fact that its loading for the fifth factor was not so small with respect to its maximum one, we may say that the Pasteur data succeed in better identify the "mysterious" previous fifth factor. In other words, we believe that the last factor of the previous analysis - that we have also pointed out as potentially difficult to interpret due to the low proportion of variance it explained - has instead been clearly identified using the data from the second school.

Finally, as a further confirmation of what we have just argued, we would like to obtain a fifth group that can be explained based on a mathematical underlying factor. The variables classified in this last group are:

```{r, echo=FALSE}
group5_past <- var.meaning %>% dplyr::filter(var %in% group5_past) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max_past), col == 5)$load.max)

group5_past
```

and the one with sufficiently high loadings:

```{r, echo=FALSE}
restricted5_past <- group5_past %>% dplyr::filter(load.max > 0.5)
restricted5_past
```

As wished, this group suits quite well the previous third one. Therefore the corresponding unobserved common factor can be reasonably called "arithmetic skills".

Before summing up what we have found, we quickly look at the variables whose loadings are all under the significance threshold value:

```{r, echo=FALSE}
doubt.var_past <- rbind(group2_past %>% dplyr::filter(load.max < 0.5), group3_past %>% dplyr::filter(load.max < 0.5), group5_past %>% dplyr::filter(load.max < 0.5))
doubt.var_past <- round(psych.5fa.ml$loadings[which(var.meaning$var%in%doubt.var_past$var),],3)
doubt.var_past <- cbind(doubt.var_past, dplyr::filter(var.meaning, var %in% row.names(doubt.var_past)))
doubt.var_past <- dplyr::select(doubt.var_past, -var)
doubt.var_past
```

We can see that for `V3` and `V18` the grouping appears to reasonably holds and that also `V16` may be actually assigned to factor 3. For what concern the last three variables things get more complicated: `V22` loads almost equally on factor 1 and 2, `V21` has also a not negligible loading on factor 3 and `V19` appears to have all low loadings.

Nevertheless we may simply address this problems referring to the transversal competences requested by the corresponding tests. The above interpretation may still be considered quite satisfactory.

To conclude, we can say that we have found quite a good agreement between the interpretations of the common factors based on the Grant-White students data and on the Pasteur student data. This is reasonable, as we expect that the underlying skills necessary to complete the proposed attitudinal tests were the same - regardless of the school attended by the students. Moreover, this concordance among the results was desirable as it can be used to assess the "stability" of our analysis and interpretation.

### 5. Make a scatterplot of the first two factor scores from the rotated MLFA solution for each school. Comment.

Exactly as we have done for the data on the students from the Grant-White school, we compute the factor scores related to the Pasteur students by means of the function `factanal`, using the varimax rotation and the regression method to compute the scores.

```{r}
psych.5fa.ml <- factanal(x=Pasteur, factors = m[1], rotation = "varimax", 
scores = "regression")
scores_Past <- psych.5fa.ml$scores
```

We then plot the first two scores. As already justified above we expect to see almost no correlation within the two scores and normally distributed data with a zero vector mean and an identity covariance matrix. Therefore we look for contour plots with an approximately circular shape and centered in the origin of the axes.

```{r, echo=FALSE}
data.frame("correlation" = cor(scores_Past[,1], scores_Past[,2]))
```

```{r, echo=FALSE}
means.hat1 <- colMeans(scores_Past[,c(1,2)])
means.hat1 <- data.frame("x_bar" = rbind(means.hat[1], means.hat1[1]), "y_bar" = rbind(means.hat[2], means.hat1[2]), row.names=c("Grant-White:","Pasteur:"))
means.hat1
```

```{r, fig.width=16, fig.height=8, echo=FALSE}
a_P <- (n-0.5)/n
S_P <- var(scores_Past)
label_P = rep("", n)

df_ell3 = data.frame(ellipse(x = S_P[c(1,2), c(1,2)], centre = c(means.hat$x_bar, means.hat$y), level = 0.95))
df_ell4 = data.frame(ellipse(x = S_P[c(1,2), c(1,2)], centre = c(means.hat$x_bar, means.hat$y), level = a_P))

pl <- ggplot(as.data.frame(scores_Past), aes(x = Factor1, y = Factor2)) + 
  geom_point() + geom_path(data=df_ell4, aes(x=Factor1, y=Factor2))

# before plotting we determine the potential bivariate outliers.
# Extract components
build <- ggplot_build(pl)$data
points <- build[[1]]
ell <- build[[2]]

# Find which points are inside the ellipse, and add this to the data
dat <- data.frame(
  points[1:2], 
  in.ell = as.logical(point.in.polygon(points$x, points$y, ell$x, ell$y))
)

label_P[!dat$in.ell] = as.character(psych$Case[which(!dat$in.ell)])

pl_sc_P <- ggplot(dat, aes(x = x, y = y)) + geom_point(aes(col = in.ell), show.legend = F) + geom_text(mapping = aes(label = label_P),
            nudge_y = -0.2, nudge_x = 0.0, size = 5, parse = T) +
geom_path(data=df_ell3, aes(x=Factor1, y=Factor2), color = "royalblue2", linetype = 2) + 
  geom_path(data=df_ell4, aes(x=Factor1, y=Factor2), color = "darkorange") + 
  scale_color_manual(values = c("red", "black")) + 
  geom_text(mapping = aes(x = 0.25, y = 2.7, label = "level = (n-0.5)/n = 0.9965"), 
            colour = "darkorange") +
  geom_text(mapping = aes(x = 0.25, y = 2, label = "level = 0.95"), 
            colour = "royalblue2") +
  geom_point(mapping = aes(x=means.hat$x_bar, y=means.hat$y_bar), 
             color = "darkorchid3", size = 3) + 
  geom_text(mapping = aes(x = means.hat$x_bar, y = means.hat$y_bar, 
                          label = "(x_bar, y_bar)"), nudge_x = 0.0, 
            nudge_y = -0.15, colour = "darkorchid3") +
  xlab("Factor 1") + ylab("Factor 2") + 
  ggtitle("Scatter-plot of the first two factor scores\n for the Pasteur students") +
  theme(plot.title = element_text(hjust = 0.5, size = 20)) +
  xlim(-3.3, 3.3) + ylim(-3.3, 3.3)

grid.arrange(pl_sc_GW, pl_sc_P, nrow = 1)
```

We again can observe that all the mentioned important features match. An outlier is identified also in the Pasteur data, corresponding to observations whose `Case` variable (in the original data set) has value 50. Globally, this plot seems to agree with the Grant-White one. Overall the two plots are very alike.

In the previous point we discussed that the first two factors for each school have the same interpretations as "verbal intelligence" and "visual, spatial and logical intelligence". Therefore we expect the plots to be very similar and to present the same distribution. The similarity of the plots support even more our analysis.

## Exercise 2

For exercise 2 we are going to consider the following dataset `pendigits`:

```{r, echo=FALSE}
rm(list=ls())
pendigits<-read.table("data/pendigits.txt", sep=",",head=F)
names(pendigits)<-c(paste0(rep(c("x","y"),8),rep(1:8,each=2)),"digit")
#dim(pendigits)
head(pendigits)
```

```{r, echo=FALSE, include=FALSE}
lookup<-c("darkgreen", "brown", "lightblue", "magenta", "purple",
"blue", "red", "lightgreen", "orange", "cyan")
names(lookup)<-as.character(0:9)
digit.col<-lookup[as.character(pendigits$digit)]
```

```{r, echo=FALSE}
n <- nrow(pendigits)
p <- ncol(pendigits)
k <- length(unique(pendigits$digit))
```

Let us start with a little data exploration in order to better understand and interpret the results below. We note that the data-set is missing some observations: we would expect $44\cdot 250=11000$ observations, but we only have $10992$. This is almost irrelevant for all future purposes, but we may need to be a bit more careful in the last sections - regarding k-fold cross validation - if we want to be accurate. We also notice that the frequencies of the digits are fairly similar:

```{r, echo=FALSE}
table(pendigits$digit)
```

so we do not expect the priors to add a strong correction term in our classifications, since they are almost uniform.

We recall that the effectiveness of linear discriminant analysis relies on the assumption of normality of the conditional distributions of the data given that they belong to one of the classes and the further assumption that all these conditional distributions share the same covariance matrix. Thus it is worth having a quick overview of how well a Gaussian model fits the conditional distribution, and how the covariance matrices compare with each other given different classes.

Let's start by assessing multivariate normality of the data via the chi-square Q-Q plot of the squared Mahalanobis distances.

```{r, fig.width= 20, fig.height=20, echo=FALSE}
pl=list(0,1,2,3,4,5,6,7,8,9)
for(i in c(0,1,2,3,5,6,7,8,9))
{
  by_digit= pendigits %>% dplyr::filter(digit==i) %>% dplyr::select(-digit)
  x.bar=colMeans(by_digit)
  S=cov(by_digit)
  d = mahalanobis(by_digit, center = x.bar, cov = S)
  pl[[i+1]]=ggplot(as.data.frame(d), aes(x = qchisq(ppoints(d), df = 16), y=sort(d))) + 
      geom_point() + geom_abline(intercept = 0, slope = 1, col = "red", lwd = 1) +
      expand_limits(x = c(-0.0, 40.0)) +
      ggtitle(paste("Chi-squared QQ-plot of\n squared generalized distances\nDigit: ",i)) +
    theme(plot.title = element_text(hjust = 0.5, size = 20))
}
grid.arrange(pl[[1]], pl[[2]], pl[[3]], pl[[4]], pl[[6]], pl[[7]], pl[[8]], pl[[9]],
pl[[10]], nrow=3)
```

We left out the particular case of the digit *4*, which has an interesting feature: the variable `y8` takes value $0$ for every observation. This implies that it cannot be a $16$-dimensional multivariate normal with "proper dimension" (the covariance matrix is only positive semi-definite and the determinant is zero), but it may still be a Gaussian vector without a pdf. To assess this, we project it in the $15$-dimensional hyperplane $\{$`y_8`$=0\}$:

```{r, echo=FALSE}
by_digit= pendigits %>% dplyr::filter(digit==4) %>% dplyr::select(-c(y8,digit))
  x.bar=colMeans(by_digit)
  S=cov(by_digit)
  d = mahalanobis(by_digit, center = x.bar, cov = S);
  ggplot(as.data.frame(d), aes(x = qchisq(ppoints(d), df = 15), y=sort(d))) + 
      geom_point() + geom_abline(intercept = 0, slope = 1, col = "red", lwd = 1) +
      expand_limits(x = c(-0.0, 40.0)) +
      ggtitle("Chi-squared QQ-plot of\n squared generalized distances") +
    theme(plot.title = element_text(hjust = 0.5))
```

It seems that all these distributions suffer, to different extents, the effects of heavier-than-expected right tails, therefore we cannot assess gaussianity of any conditional distribution.\
We proceed with the analysis, keeping in mind that, even if the assumptions fail, the robustness of LDA could guarantee fairly good results.

Now we compare variances. To check that the conditional densities all share the same covariance matrix, we compute the Frobenius distances between each class covariance matrix and the pooled covariance matrix, which is ultimately the one used in LDA as estimator of the - allegedly - shared population covariance matrix among classes.

```{r}
covar=list()
for(i in 1:10){
  by_digit= pendigits %>% dplyr::filter(digit==(i-1)) %>% dplyr::select(-digit)
  covar[i]=list(cov(by_digit))
}

#pooled sample covariance matrix
n.k<-as.numeric(table(pendigits$digit))
out<-by(pendigits[,1:16],pendigits$digit,var)
W<-matrix(0,ncol=16,nrow=16)
for (i in 1:k) W<-W+out[[i]]*(n.k[i]-1)
S<-W/(n-k)

f=vector()
for(i in 1:10){
      f[i]=frobenius.norm(covar[[i]]-S)
}
data.frame(digit=0:9, frobenius=f)
    
```

We find that there are some digits, namely *5*, *8*, *0* and *1*, for which the assumption of equal class covariance matrix might be more "stretched" than for the others. We keep this in mind for future reference, as this might be linked to worse performance of the classification of these digits. Even more so if we consider that also the normality assumption seemed to be unfitting, so we lack theoretical guarantees on a good performance of LDA.

### 1. Use linear discriminant analysis (LDA). Display the first two LD variables in a scatterplot, color coding the observations according to variable digit.col below. How well do they discriminate the 10 digits? Refer also to theory.

First of all, as requested, we proceed by implementing the linear discriminant analysis:

```{r}
lda.fit <- MASS::lda(digit ~ ., data = pendigits)
lda.fit
```

Note that, by default, the priors are the proportions of occurrences of each class in the data set, and they are all pretty similar.

```{r}
lda.pred <- predict(lda.fit)
```

```{r, echo=FALSE}
lda.pred$x <- as.data.frame(lda.pred$x)
means.hat <- aggregate(lda.pred$x,by=list(pendigits$digit),FUN=mean)[,-1]
```

We can now display the first two LD variables in the below scatterplot, which gives the "optimal" bi-dimensional representation of the data for "separation purposes" (i.e. with minimized within-class variance and maximized between-class variance). Note that the assumptions under which this optimality holds, as previously discussed, are not really met, but we still expect a good result:

```{r, fig.height=12, fig.width=18, echo=FALSE}
pl <- ggplot(as.data.frame(lda.pred$x), mapping = aes(x = LD1, y = LD2)) + 
  geom_point(aes(colour = factor(pendigits$digit)), alpha = 0.5, show.legend = T) +
  stat_ellipse(aes(colour = factor(pendigits$digit))) +
  scale_color_manual(values = c("darkgreen",  "brown", "lightblue",  "magenta", "purple", "blue", "red", "lightgreen", "orange", "cyan")) +
  labs(colour = "digit") + 
  geom_point(as.data.frame(means.hat), mapping = aes(x = LD1, y = LD2, colour = factor(pendigits$digit)), 
             shape = 21, colour = "black", fill = lookup, size = 5, stroke = 1.0) +
  ggtitle("scatter-plot of the first two LD variables")+
  theme(plot.title = element_text(hjust = 0.5, size = 30))
pl
```

An analysis of the considered scatterplot should therefore give an idea of the overall behavior of the classifier. However, since the proportion of trace of the first two LDs is just around $0.6$, we do not expect the information provided by the plot to be entirely accurate for a full rank LDA.\
In the plot we can appreciate that the digit *4* forms a clear and compact cloud at the top, and so does *2* at the left. Meanwhile the other digits have clouds with some values far away from the centroids (the points highlighted by the black boundaries). The worst case seems to be digit *5*, which appears to have somewhat of a "bi-modal projected distribution", with points concentrated in two separated regions. Digits *0* and *8* show some overlap with the neighboring ones, and *1* even more than them. In general all the other digits do not form a compact distribution around their centroids and display very different variances in the LD1-LD2 plane.

We can also observe that the first two discriminant directions do a quite good job in separating the class centroids, as their associated coordinates on the respective axes are all quite different from one another. In particular, LD1 seems to "isolate" quite well the centroids associated to *0* and *2*, whereas LD2 does the same for *4* and *8*. Conversely, *1*, *3* and *7* appear to be all "clustered together" by the first two LDs.

We can start making some speculations, that we will investigate better in the analysis below.\
Digit *5* could be confused with *9* due to their considerable overlapping area: we could have suspected it, considering how similar their shapes are. The same goes for *8* and *0*.\
This outcome could arise from the fact that the assumptions under which the optimal discrimination holds, as previously shown, are not fully met, as well as the fact that the proportion of trace explained by the first two LDs is not satisfactory.

### 2. Compute the confusion matrix on the training data. What are the groups more difficult to discriminate from the others? Comment in view of the answer to point 1.

In order to have a better understanding of how well the first two LD variables discriminate the ten digits, we compute the confusion matrix on the training data:

```{r}
conf.mat <- table(predicted=lda.pred$class,true=pendigits$digit)
conf.mat <- addmargins(conf.mat)
conf.mat
```

We also print the misclassifications for each digit:

```{r, echo=FALSE}
#A=matrix(data=NA,nrow=10,ncol=2)
#conf.mat_2=conf.mat[-11,]
#for(i in 1:10){
#  A[i,2]=sum(conf.mat_2[-i,][,i])
#}
#A[,1]=c(0,1,2,3,4,5,6,7,8,9)
#A
#another way: 
B = conf.mat[11,-11]-diag(conf.mat[-11,-11])
mis = data.frame("misclassifications"=B, "misclassification.rates"=round(B/conf.mat[11,-11],3))
mis
```

```{r, echo=FALSE}
digits <- rownames(conf.mat)[-11]
true_num <- conf.mat[11,1:10]
predicted_num <- conf.mat[1:10,11]
predictions <- data.frame(digit = digits, true_num = true_num, predicted_num = predicted_num)
predictions <- melt(predictions)
p1 <- ggplot(predictions, aes(x = digit, y = value, fill = variable)) + 
  geom_bar(stat = "identity", position = "dodge") + 
  scale_fill_manual(values = c("royalblue3", "darkorange"), 
                    labels = c("true", "predicted"), name = "count per digit")
#misclassifications <- conf.mat[11,1:10] - diag(conf.mat)[-11]
misclassifications <- data.frame(digit = digits, mis=mis$misclassification.rates)
p2 <- ggplot(misclassifications, aes(x = digit, y = mis, group = 1, label = mis)) + geom_label_repel() +
  geom_point(size = 3) + geom_line() + 
  ggtitle("misclassifications rates for each digit") + 
  theme(title = element_text(hjust = 0.5))
grid.arrange(p1, p2, nrow = 2)
```

In the confusion matrix we can see similar patterns to the scatterplot: the digit *4* is well discriminated, it has only 28 misclassifications out of 1144 occurrences. It is interesting to note that, as previously speculated, the digit *5* has been wrongly predicted as *9* 268 times and the digit *0* as *8* 77 times. We also remark that the digit *5* has the highest misclassification rate, as suggested by the scatterplot. Another interesting feature that arises from the confusion matrix is how some digits are misclassified predominantly in one direction. For example the digit *1* has been misclassified as *2* $206$ times, meanwhile the digit *2* as *1* only $23$ times. This was already suggested by the scatterplot: due to the high variance of the digit *1* a lot of its observations are close to the centroid of the digit *2* and so they may end up being classified as $2$ by the nearest centroid classification. This clearly does not apply in the opposite direction.\
Finally, note that the digits that are misclassified more often are also those that we found to have a class covariance matrix "more distant" from the pooled one compared to those with better misclassification rates.

### 3. Use the leave-one-out cross validation (CV). Compute the confusion matrix and the corresponding CV errors. Is it larger than the training error? Why so?

As requested, we proceed with the leave-one-out cross validations:

```{r}
lda.fitCV<-lda(digit~.,data=pendigits, CV=TRUE)
```

And we compute the corresponding confusion matrix.

```{r}
conf.mat<-table(predicted=lda.fitCV$class,true=pendigits$digit)
addmargins(conf.mat)
```

We compare the training error of LDA without CV, with the one achieved via leave-one-out cross validation:

```{r}
training.err<-1-mean(lda.pred$class==pendigits$digit)
CV.err<-1-mean(lda.fitCV$class==pendigits$digit)
data.frame("training error"=training.err, "CV error" = CV.err)
```

As we can see, the test error rate achieved via leave-one-out cross validation turns out to be bigger than the training error. This is to be expected: the training error is a very over-confident estimate of the Actual Error Rates, because it tests its performance on the same data used to train it (so it suffers a form of "overfitting" to the data). The leave-one-out CV procedure instead tests models, trained on all data minus a single observation, on that observation and averages the error rates obtained by repeating this procedure on each observation. It is a more realistic measure of how well the model would fare with unseen data (and thus tends to give a less-optimistic approximation of the AER).\
Still, the difference of the rates is not very appreciable. This might have to do with the fact that leaving a single handwritten digit out has only a marginal impact on the predictive capability of that digit as the model is trained on (around) 249 other digits from the same writer - plus the ones of all other 33 authors.

### 4. Compute the $44$-fold cross validation error for each reduced-rank LDA classifier, including full-rank LDA, by using the partition of the observation provided by the variable `groupCV` below. Plot the error curve against the number of discriminant variables. What classifier do you prefer? Comment.

```{r, echo=FALSE}
groupCV<-rep(1:44, each=250)
groupCV<-groupCV[1:length(pendigits$digit)]
```

Let's start by observing that $44$ is also the number of different writers who contributed to writing the digits for the training data. If we were under the assumption that the observations were grouped in order of their "author", this $44$-fold cross validation could turn out to be a very effective way of measuring the predictive capability of our model: we would effectively test how well observation from an "unseen" (as in: left out from the training) writer are classified based on the training data of the remaining $43$. However, given the information available on the data-set, we cannot be sure that this is the case.\
In the following section we will use two different $44$-fold cross validation error estimates: one that takes a simple average of the error rates over all folds, and one that takes an average of those fold-related error rates weighted on the amount of test data of said folds, to account for the fact that the last group of test data is slightly smaller than the others. The latter is, in essence, the total misclassification rate of the model: it simply amounts to dividing the total number of misclassifications over all the folds and dividing it by the size of the dataset. Since the difference in size of the last fold is of only $8$ observations out of $250$ we do not expect a sensible difference.

```{r}
k_f=44

# Initialize error rates vector
err=matrix( nrow=k_f, ncol=min(k-1,p) )
mis=rep(0, min(k-1, p))
train_err = rep(0, min(k-1, p))

#Loop over folds
for (i in 1:k_f) {
  # Define test and train sets
  test=(groupCV == i)
  train=!test
  
  # Fit LDA model on training data
  lda.fit = lda(digit~.,data=pendigits[train,])
  
  for(j in 1:min(k-1,p)){
    # Predict test data using LDA model
    lda.pred = predict(lda.fit, pendigits[test,], dimen = j)$class
    lda.pred_train = predict(lda.fit, pendigits[train,], dimen = j)$class
    
    # Compute error rates
    train_err[j] = train_err[j]+sum(lda.pred_train != pendigits$digit[train])
    mis[j] = mis[j]+sum(lda.pred != pendigits$digit[test])
    err[i,j] = 1 - sum(lda.pred == pendigits$digit[test])/length(lda.pred)
  }
}
error44=colMeans(err)
mis_rate44=mis/n
train_err44 = train_err/((k_f -1)*n)

data.frame("rank"=1:9, "training errors" = train_err44, 
       "arithmetic mean of errors"=error44,
"misclassification rate"=mis_rate44)
```

As expected, the differences are hardly noticeable.

```{r, fig.height=12, fig.width= 18, echo=FALSE, warning=FALSE}
dataf=data.frame("rank" = as.character(9:1), "training_error" = train_err44[9:1], "arithmetic_mean" = error44[9:1], "misclassification_rate" = mis_rate44[9:1])
dataf <- melt(dataf)

ggplot(dataf, aes(x=rank, y=value, label = as.character(round(value, 4)), color = variable, group = variable, shape = variable)) + geom_point(size = 3) + 
  geom_line() + geom_label_repel(show.legend = F) +
  scale_x_discrete(limits=levels(factor(1:9))) +
  scale_color_manual(values = c("red", "darkorange", "royalblue3")) +
  ggtitle("arithmetic error mean and misclassification rate for the test data\n and training error") +
  theme(plot.title = element_text(hjust = 0.5, size = 30))
```

We can see that the cross validation error keeps improving as the number of dimensions grows, which suggests that all linear discriminant directions matter for classification purposes - albeit to different extents. We could make a case for keeping only 6 dimensions if we focused on dimensionality reduction, following a sort of "elbow rule". Overall, the most balanced decision is to keep all 9 dimensions.

### 5. $(optional)$ Find a classification rule that improves on the CV error rate estimates found before. Feel free to use any classification method, even one not covered in class.

Since the assumption of equal covariance matrix across classes seems stretched, our first thought is simply to try Quadratic Discriminant Analysis, which doesn't require it. Note that we wouldn't expect a huge improvement, since it would still rely on the assumption of Gaussian conditional densities.

QDA is, however, impossible to implement in this case. Indeed, QDA requires invertibility of all the sample covariance matrices in its procedure, not only of the pooled one, and digit *4* doesn't have a full rank covariance matrix (as previously discussed).

```{r, error=TRUE}
qda.fit=qda(digit~.,data=pendigits, CV=TRUE)
```

Before switching to another framework, we could try and implement a similar procedure that ignores the problem of the singularity of the covariance matrix of digit *4*: Regularized Discriminant Analysis. This technique is in between LDA and QDA, in the sense that it relies on class covariance matrix estimates that are given by a linear combination of the pooled covariance matrix and of the sample covariance matrix of the considered class. The coefficients of the combinations are estimated as to minimize the error rates by default. Quite unexpectedly, the improvements achieved via a 44-fold Cross Validation RDA on the LDA error rates are astounding - where we used again a 44-fold cross validation technique to make error estimates more comparable with the previous ones.

```{r}
rda.fit=rda(digit~.,data=pendigits, crossval=TRUE, fold=44)
rda.fit$error.rate
```

Here "APER" (apparent error rate) corresponds to what we called "total misclassification rates" above, and "crossval" is the cross validation error. The errors are almost reduced by a $\frac{1}{10}$ factor compared to LDA.

Since relying less on the assumptions of LDA provides us an improved outcome, the next technique we implement is Multinomial Logistic Regression (a generalisation of the standard Binomial Logistic Regression for categorical variables with more than 2 possible outcomes). Indeed, the strategy is almost identical to the one of LDA (maximization of the discriminant functions), but it relies on the weaker assumptions of logistic regression (the log-odds of each class is assumed to be a linear combination of the explanatory variables). We repeat k-fold CV on this model using the same criterion above.

```{r, results=FALSE}
mlog.err=0
mlog.mis=0
for(i in 1:k_f){
  set.seed(i) #random initial parameters, different for each cycle
  train = !(groupCV==i)
  train.data=pendigits[train,]
  test.data=pendigits[!train,]
  mlog.model = nnet::multinom(digit ~., data = train.data)
  pred.classes = mlog.model %>% predict(test.data)
  mlog.err=mlog.err+1-mean(pred.classes == test.data$digit)
  mlog.mis=mlog.mis+sum(!train)-sum(pred.classes == test.data$digit) 
  #number of misclassifications as the total number of test data - the correctly predicted class
}
mlog.err=mlog.err/k_f #error by means of the errors

mlog.mis=mlog.mis/n
```

```{r, echo=FALSE}
data.frame("Arithmetic average of errors"=mlog.err, 
           "Misclassification rate"= mlog.mis)
```

The results are good, but not as good as those of RDA. Furthermore, the function used to implement this Multinomial Logistic Regression relies on a random choice of initial parameters and the outcome is sensitive to the choice of these initial values.

At last, we attempt to use a random forest, a popular classification method that performs very well on many classification tasks. Usually the great advantage of random forests is the interpretability of the model, which in this case is quite secondary, but it's still worth checking if the errors improve:

```{r}
rfor.err=0
rfor.mis=0
for(i in 1:k_f){
  set.seed(i)
  train = !(groupCV==i)
  train.data=pendigits[train,]
  train.data$digit=as.factor(train.data$digit)
  test.data=pendigits[!train,]
  test.data$digit=as.factor(test.data$digit)
  rfor.model = randomForest(digit ~., data = train.data, ntree=100)
  pred.classes = predict(rfor.model, test.data)
  rfor.err=rfor.err+mean(pred.classes != test.data$digit)
  rfor.mis=rfor.mis+sum(pred.classes != test.data$digit) 
}
rfor.err=rfor.err/k_f #error as means of the errors
rfor.mis=rfor.mis/n

data.frame("Arithmetic average of errors" = rfor.err,
           "Misclassification rate" = rfor.mis) 
```

Random forests perform better than all previously considered methods.

At last, we mention that we also tried to use the more "naive" method of the k-Nearest Neighbors, which yielded surprisingly good results for small values of k. We show the error rates for k from $1$ to $10$, but up to $100$ they only increased along with k, so the optimal choice appear to be either $1$ or $3$. This behavior was not completely predictable: the error rate is usually an increasing function of k when computed on training data, yet it still happens the same despite testing the model on test data - not used for training - via cross-validation. 

```{r}
set.seed(1)
knn.err=numeric(10)
knn.mis=numeric(10)
for(aux in 1:10){
  knn.err_tmp=0
  knn.mis_tmp=0
  for(i in 1:k_f){
    train = !(groupCV==i)
    train.data=pendigits[train,]
    train.data$digit=as.factor(train.data$digit)
    test.data=pendigits[!train,]
    test.data$digit=as.factor(test.data$digit)
    knn.pred = class::knn(train=train.data[,1:16], cl=train.data[,17],
                          test=test.data[,1:16], k=aux)
    knn.err_tmp=knn.err_tmp+mean(knn.pred != test.data$digit)
    knn.mis_tmp=knn.mis_tmp+sum(knn.pred != test.data$digit) 
  }
  knn.err[aux]=knn.err_tmp/k_f
  knn.mis[aux]=knn.mis_tmp/n
}
```

```{r, echo=FALSE}
data.frame("k"=1:aux, "Arithmetic average of errors" = knn.err,
"Misclassification rate" = knn.mis) 
```

In terms of raw error rates, k-NN should be our best choice. However, due to its instability, we should probably opt for Random Forests regardless.