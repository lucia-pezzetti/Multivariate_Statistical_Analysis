---
title: "ProblemSet2"
format: pdf
editor: visual
---

```{r}
rm(list=ls())
```

```{r}
library(plyr)
library(tidyverse)
library(MASS)
library(ggplot2)
library(ellipse)
library(corrplot)
library(MVN)
library(sp)
```

## Exercise 1

We consider the dat set psych which contains 24 cognitive tests administered to 301 students (with ages ranging from 11 to 16) in a suburb of Chicago: a group of 156 students (74 boys, 82 girls) from the Pasteur School and a group of 145 students (72 boys, 73 girls) from the Grant-White School.

```{r}
## psych data
#setwd("c:/Users/lucia/OneDrive/Desktop/Documenti/Stochastics and Data Science/Multivariate Statistical Analysis/PS2/problemset_2/problemset_2")
psych<-read.table("data/psych.txt",header=T)
dim(psych)
head(psych)
with(psych,table(group))
```

```{r}
var <- colnames(psych[,4:27])
meaning <- c("visual perception", "cubes", "paper form board", "flags", "general information", "paragraph comprehension", "sentence completion", "word classification", "word meaning", "addition", "code", "counting dots", "straight-curved capitals", "word recognition", "number recognition", "figure recognition", "object-number", "number-figure", "figure-word", "deduction", "numerical puzzles", "problem reasoning", "series completion", "arithmetic problems")
var.meaning <- as.data.frame(t(rbind(var, meaning)))
```

#### Use the Grant-White students data. Obtain the maximum likelihood solution for m = 5 and m = 6 factors and compute the proportion of total sample variance due to each factor. List the specific variances, and assess the accuracy of the approximation of the correlation matrix. Compare the results. Which choice of m do you prefer? Why?

First of all we start by filtering the data set in order to retain only those observations corresponding to students from the Grant-White School. Once we have done so, we can remove the variable group from the data set we are going to use in the analysis. Together with group we also discard the variable *Case*, that simply enumerate the collected observations. As a side-note we also observe that the total number of considered cases is 350 whilst we only have 301 observations, suggesting that some students did not attend the test or maybe responded only partially. Finally we convert the variable *Sex* into a binary 0-1 variable.

```{r}
Grant_White <- psych %>% dplyr::filter(group == "GRANT") %>% dplyr::select(-c(Case, group)) %>% dplyr::mutate(Sex = as.numeric(revalue(Sex, c(F = 1, M = 0))))
```

The aim of the Factor Analysis is, in essence, to describe - if possible - the covariance relationships among many variables in terms of a few underlying, but unobservable, random variables called factors. Loosely speaking the idea is the following: suppose that the considered variables can be grouped according to their correlation pattern - i.e. that we can cluster the variables in such a way to have high intra-groups correlations and small inter-groups correlations. It seems reasonable to believe that each group of variables may be linked to a single underlying and unobservable factor, which ultimately is the responsible for the observed significant intra-group correlations.

Looking at the psych data set it is conceivable that the results of the "ability" tests performed are just different measures of fewer broad domains like for example verbal or spatial abilities, memory or mathemathical deduction.

We start our analysis by considering the correation matrix:

```{r}
R <- cor(Grant_White)
#testRes = cor.mtest(Grant_White, conf.level = 0.95)
```

It is helpful to plot it:

```{r, fig.width = 13, fig.height = 13}
corrplot(R, method = 'color', type = 'lower', insig='blank',
         addCoef.col ='black', number.cex = 0.8, diag=FALSE, col = COL2('RdBu', n = 200))
```

-----------BOH------------As we can observe the variables *Sex* and *Age* are low correlated within each other and within all the other variables. This seems to suggests that neither of these variables have a relevant role in affecting the results of the tests. Moreover it seems unlikely to us that sex and age of a student may be explained by some underlying factors. For these reasons, we will consider only the results of the 24 tests.------------------------------

```{r}
#col <- plyr::revalue(Grant_White$Sex, c(0 = "magenta", 1 = "blue"))
Grant_White <- Grant_White %>% dplyr::select(- c(Age,Sex))
Grant_White <- scale(Grant_White)
n <- nrow(Grant_White)
p <- ncol(Grant_White)
R <- cor(Grant_White)
```

In particular, we are asked to perform a Factor Analysis with $m=5$ and $m=6$ common factors using the maximum likelihood approach. In R we can rely on the function *factanal*, however in applying this function rely on the strong assumption of normality of the data. Therefore we start by assessing this assumption.

To do so we plot the chi-square qq-plot of the squared Mahalanobis distances, under the assumption of multivariate normality, the data will fall along the bisector. 

```{r}
x.bar <- colMeans(Grant_White)
S <- cov(Grant_White)
d = mahalanobis(Grant_White, center = x.bar, cov = S);
ggplot(as.data.frame(d), aes(x = qchisq(ppoints(d), df = p), y = sort(d))) + 
 geom_point() + geom_abline(intercept = 0, slope = 1, col = "red", lwd = 1) +
  expand_limits(x = c(9.0, 60.0), y = c(9.0, 60.0)) +
  ggtitle("Chi-squared QQ-plot of squared generalized distancees")
```
As we can see from the plot, the points deviate from the straight line only in the right top end. This indicates that there the points are more dense than for a $\chi_{24}^2$. Correspondingly, since the largest values are more spread out, the tail of the chi-square distribution is more heavier. So, the plot seems to suggest that the Mahalanobis' distances in the sample are more right skew than you would expect to see with a mutivariate normal. 

Nevertheless, we also observe that the points responsible for this potential non-gaussian behaviour are only three, hence they can simply be multivariate outliers.

```{r}
a <- (n-0.5)/n
x = 1:length(d)
outliers <- which(d > qchisq(a, df = p))
labels <- rep("", n)
labels[outliers] <- as.character(outliers)
colors <- rep("black", n)
colors[outliers] <- "red"
ggplot(as.data.frame(d), aes(x = x, y = d, label = labels)) + 
  geom_point(pch = 16, col = colors) + 
  geom_text(hjust = 0.2, nudge_x = -0.05, nudge_y = 2.0, size = 4) +
  geom_hline(yintercept = qchisq(a, df = p), col = "red", lty = 2) + 
  ggtitle("Multivariate outliers") + 
  ylab("squared Mahalonobis distances") + 
  xlab("indexes") + 
  theme(plot.title = element_text(hjust = 0.5))
```
As we can immediately see from the plot, observations 12 and 24 are multivariate ouliers: It is then customary to temporaliry remove them and study again the mutlivariate normality of the dataset.

```{r}
without_outliers <- Grant_White[-c(12,24),]
x.bar <- colMeans(without_outliers)
S <- cov(without_outliers)
d = mahalanobis(without_outliers, center = x.bar, cov = S);
ggplot(as.data.frame(d), aes(x = qchisq(ppoints(d), df = p), y = sort(d))) + 
 geom_point() + geom_abline(intercept = 0, slope = 1, col = "red", lwd = 1) +
  expand_limits(x = c(9.0, 60.0), y = c(9.0, 60.0)) +
  ggtitle("Chi-squared QQ-plot of squared generalized distancees without outliers")
```
The chi-square qq-plot of the Mahalanobis distances now seems to fit well the assumption of multivariate normality.

As a final further test, we list the result of the Henze-Zirkler's multivariate normality test and of the Mardia's multivariate skewness and kurtosis coefficients as well as their corresponding statistical significance. The tests are performed using the function mvn in the MVN package. We firstly test the whole data set:

```{r}
#mvShapiro.Test(as.matrix(scale(Grant_White)))
multnorm_hz <- mvn(Grant_White, scale = T)
multnorm_hz$multivariateNormality
multnorm_mardia <- mvn(Grant_White, mvnTest = "mardia", scale = T)
multnorm_mardia$multivariateNormality
```
as already highlighted using the chi-square qq-plot, the problem seems to lie in the skewness of the data. If then we re-do the analysis without taking into account the outliers:
```{r}
multnorm_hz <- mvn(without_outliers, scale = T)
multnorm_hz$multivariateNormality
multnorm_mardia <- mvn(without_outliers, mvnTest = "mardia", scale = T)
multnorm_mardia$multivariateNormality
```
All problems seems to disappear and multivariate normality can be taken for granted.

We can now legitimately proceed with the maximum likelihood approach to perform the Factor Analysis. As requested, we consider and compare the choices of $m=5$ and $m=6$ factors without applying, for now, any rotation.

```{r}
m = c(5,6)
psych.5fa.ml <- factanal(x=Grant_White, factors = m[1], rotation = "none")
psych.6fa.ml <- factanal(x=Grant_White, factors = m[2], rotation = "none")
```
We start by looking at the obtained loadings, in the case of 5 factors:
```{r}
L.5.ml <- psych.5fa.ml$loadings; L.5.ml
```
and of 6:
```{r}
L.6.ml <- psych.6fa.ml$loadings; L.6.ml
```


In particular the proportion of variance explained by each factor is computed as the sum of the squared loadings of the considered factor divided by the number of variables p. We get the following results:

```{r}
L5 <- psych.5fa.ml$loadings
L6 <- psych.6fa.ml$loadings
sL5 <- colSums(L5^2)
sL6 <- colSums(L6^2)
rbind(`Proportion Var` = round(sL5/p, 3L))
rbind(`Proportion Var` = round(sL6/p, 3L))
```
As we have seen in the lectures, the proportion of variance explained by each factor indicates how much of the total variability in the original variables is accounted for by that particular factor. Specifically, it represents the proportion of the total variance in the observed variables that can be attributed to that factor. For this reason it is usually used to address the relevance of the factors and, consequently, as a support to the choice of the number of factors to retain.

As a general guideline we can say that factors that explain a large proportion of the variance in the original variables are considered more important and may be more useful for further analysis. Conversely, factors that explain a small proportion of the variance may be less useful and can potentially be dropped from further analysis. 

Nevertheless it is important to always keep in mind that there is not a univocal way to decide how many factors should we retain. Each situation need to be analyzed separately and other factors - such as interpretability and theoretical considerations - may play a relevent role.

--------------- vi sembra sensato? --------------------------------------------------------
In our situation we can observe that the first factor explains quite a relevant percentage - the 30% - of the total variability of the data. Therefore we expect it to be in some sense the most important of all factors and to represent an "important" psychological construct or ability. The other factors account for a much smaller proportion. Surely the less explanatory ones are the fifth and the sixth - when present - factors, as they respectively explain the 2.2/2.1% and the 1.7% of the total variance. In particular, the fact that they explain a relatively small but similar proportion of variance seems to suggest that we should reserve the same treatment to both of them - i.e. retain or discard them both. However, as said before, the decision to discard a factor should not be based solely on the proportion of variance it explains, hence before making any further consideration we proceed in our analysis.
----------------------------------------------------------------------------------------------

Given the proportion of variance accounted for by each factor, we can also compute the cumulative proportion of variance explained by the factors. To do so we simply apply the function cumsum to the single proportion of variances.

```{r}
rbind(`Cumulative Var` = cumsum(round(sL5/p, 3L)))
rbind(`Cumulative Var` = cumsum(round(sL6/p, 3L)))
```
As we can see, both the factor analysis performed with 5 factors and the one with 6 common factors only explain slightly more than the 50% of the total variability of the data. Therefore, they don't seem to be a good fit for the data.

Now, in a real-life study, there are several things we can do to try improve the fit of the model, some examples include:
- removing the outliers from the data set and re-estimate the maximum likelihood solution;
- using different methods to perform the factor analysis - for example a possibility is to use the principal component approach
- finally, the last possibility is to consider alternative analyses - such as principal component analysis, cluster analysis, or discriminant analysis.

We won't try neither of these improvement, but - for the sake of the exercise - will proceed in the factor analysis, we will however take this issue into account during the whole analysis.


Another relevant features to examine are the specific variances, or uniquenesses. The $i^{th}$ uniqueness represents the proportion of variance of the $i^{th}$ variable that is not accounted for by the factors. In other words, the specific variances are important because they provide information about the unique contribution of each observed variable to the total variance. Variables with large specific variances are those that are not well accounted for by the underlying factors. These variables are also more likely to have weaker factor loadings and may be more difficult to interpret. On the other hand, variables with small specific variances are those that are well explained by the factors and are more likely to have stronger factor loadings.

For all these reasons, the specific variances can also affect the decision of how many factors to retain in the analysis: if there are many observed variables with high specific variances, this may indicate the necessity to add some extra factors.

The specific variances can also be used to assess the overall fit of the factor model. If the specific variances are very large, this may indicate that the model is not a good fit for the data and may need to be revised. Alternatively, if the specific variances are very small, this may indicate that the model is overfitting the data and may need to be simplified.

Overall, specific variances are an important part of the output of factor analysis and can provide valuable information about the quality of the factor model and the interpretation of the results.

We list our results:

```{r}
uniq <- cbind(psych.5fa.ml$uniquenesses, psych.6fa.ml$uniquenesses)
uniq <- as.data.frame(round(uniq, 3))
colnames(uniq) <- c("uniquenesses_nfactors_5", "uniquenesses_nfactors_6")
uniq
```
As we could expect by the fact that the total variance explained by the common factors is quite low, the uniquenesses are generally quite high. Moreover, there are no variables with a specific variance that can be considered sufficiently small: the variable that is best explained by the factors has the same a uniqueness greater than 0.2.

We are mainly interested in two different aspects:
- high specific variances (or very low ones, but there isn't any in our case);
- significant difference among the uniquenesses returned using 5 or 6 factors.

For what concern the higher specific variances, we can observe that there are 16 variables for which both of their uniquenesses are higher than 0.4, specifically:
```{r}
uniq %>% dplyr::filter(pmin(uniquenesses_nfactors_5, uniquenesses_nfactors_6) > 0.4)
```
Again, the presence of such a great number of variables with high uniquenesses may suggests that the model does not fit the data very well.

Indeed, high uniqueness values in a factor analysis indicate that a large proportion of the variance in a variable is not accounted for by the factors extracted from the analysis. This means that the variable is not well explained by the underlying factors and has a unique contribution to the overall variance in the data.

----------------- Si potrebbe togliere in realtà ----------------------------------------------
Keeping this in mind, it is also important to note that high uniqueness values are not necessarily a problem in themselves. Some variables will naturally have higher uniqueness values than others, depending on their level of specificity or measurement precision. Therefore one should always consider the specific context of the phenomenon being studied.

In our analysis, the presence of such a high number of variables with high uniqueness value may not be completely unexpected: the data are related to aptitude tests on students, thus it seems reasonable to expect that the tests also rely heavily on individual abilities of pupils that, the common factor would fail to encode.

Nevertheless, the number of factor with a great amount of unexplained variability seems to us too high to be ignored, even if we keep into account the individual characteristics of the students.
------------------------------------------------------------------------------------------------

Let's now look at the variables for which the inclusion of the sixth factors plays a relevant role:

In particular we decided to print the specific variances of the variances for which we the addition of the sixth factor lead to a decrease - or increase - of the specific variance greater than 0.1. We also return them in decreasing order: the first variable returned is the one with the greatest gap among the two uniquenesses.

```{r}
uniq %>% dplyr::filter(abs(uniquenesses_nfactors_5 - uniquenesses_nfactors_6) > 0.1) %>% dplyr::arrange(desc(uniquenesses_nfactors_5 - uniquenesses_nfactors_6))
```

As we can observe, the variables that are mainly affected by the introduction of a new factor are *V17* - corresponding to a test whose target is the ability to memorize object-number associations - and *V11* - a speeded code-test that consists in transforming shapes into alpha with code. So we expect these two variables to be better explained by 6 common factors than 5.

It is also worth-noting that the number of variables for which the uniquenesses is significantly reduced by the introduction of the sixth factor is only two. This may suggest that the model with 6 factors may fail in providing a relevant improvement with respect to the one with five factors.

We can now assess the approximation of the correlation matrix. To do so we can compute the residual matrix
$$
\begin{aligned}
  \mathbb{R} - \hat{\mathbb{L}}\hat{\mathbb{L}}^T - \hat{\mathbb{\Psi}}
\end{aligned}
$$

resulting from the approximation of $\mathbb{R}$ via the simpler structure $\hat{\mathbb{L}}\hat{\mathbb{L}}^T - \hat{\mathbb{\Psi}}$. We can then summarize how far from the perfect approximation we are by computing its Froboenius norm.

```{r}
Psi.5.ml <- diag(psych.5fa.ml$uniquenesses, p)
Psi.6.ml <- diag(psych.6fa.ml$uniquenesses, p)
Residual.5.ml <- R - (L.5.ml%*%t(L.5.ml) + Psi.5.ml)
Residual.6.ml <- R - (L.6.ml%*%t(L.6.ml) + Psi.6.ml)
Frob.res <- cbind(sum(Residual.5.ml^2), sum(Residual.6.ml^2))
row.names(Frob.res) <- "Frobenius norm of the residual matrix: "
colnames(Frob.res) <- c("nFactors_5", "nFactors_6")
as.data.frame(round(Frob.res, 3))
```
The obtained norms - for the choice of 5 and 6 factors - are in both cases quite high, even if adding the sixth factor slightly reduces it. This may again be explained by keeping into account the fact that in both cases the total cumulative variance explained by the factors is quite small - 0.503 and 0.525 respectively. Therefore, we can conclude that, despite the improvement in the approximation related to the inclusion of the sixth factor, in both cases the approximation of the correlation matrix is not so satisfactory.

Overall, the comparison seems to suggests that neither the 5 factors model nor the 6 factors one are good fit for the data. 
Anyway, if we stick to the problem of deciding how many factors to retain, the choice of 5 common factors may appear to be the most preferable one. Indeed, as it emerges from all the comment above, the improvements related to the use of six factors are not sufficiently noteworthy.


## Give an interpretation to the common factors in the m = 5 solution with varimax rotation

EXPLAIN WHY WE USE VARIMAX ROTATION

```{r}
psych.5fa.ml <- factanal(x=Grant_White, factors = m[1], rotation = "varimax")
psych.5fa.ml$loadings
```

In order to interpret the factor we classify the observed variables into five groups - each one corresponding to a different factor - based on their loadings. 

As a first subdivision we simply choose to assign each variable to the factor corresponding to its greatest loading. Then we assess the significance of the loadings using a threshold value equal to 0.5.

```{r}
# il codice non è proprio giusto, non funziona se ci sono due loadings sopra 0.5, qui non è il caso quindi potremmo semplicemente nasconderlo... avevo importato tutto cercando il max
maximum <- apply(psych.5fa.ml$loadings, 1, max)
pos.max <- as.data.frame(which(psych.5fa.ml$loadings == maximum, arr.ind = T))
pos.max <- pos.max %>% dplyr::mutate(load.max = round(psych.5fa.ml$loadings[as.matrix(pos.max)], 3))
group1 <- row.names(dplyr::filter(as.data.frame(pos.max), col == 1))
group2 <- row.names(dplyr::filter(as.data.frame(pos.max), col == 2))
group3 <- row.names(dplyr::filter(as.data.frame(pos.max), col == 3))
group4 <- row.names(dplyr::filter(as.data.frame(pos.max), col == 4))
group5 <- row.names(dplyr::filter(as.data.frame(pos.max), col == 5))
```

The group of variables associated to the first factor is given by:
```{r}
group1 <- var.meaning %>% dplyr::filter(var %in% group1) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max), col == 1)$load.max)
group1
```
We can immediately notice that the considered loadings of all the variables in this group have an enough level of significance.

The interpretation of this factor seems also quite straightforward, as all the variables associated to it are related to some kind of linguistic and comprehension abilities. The tests indeed are devised to target students capacity to understand the meaning of a phrase or a word as well as their aptitude to correctly identify the class of a word or complete a sentence.

For this reason we may denote it as "verbal intelligence".


The group of variables associated to the second factor is:
```{r}
group2 <- var.meaning %>% dplyr::filter(var %in% group2) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max), col == 2)$load.max)
group2
```
Based on our significance threshold value, only the following four variables should be retained:
```{r}
restricted2 <- group2 %>% dplyr::filter(load.max > 0.5)
restricted2
```

The variables *V1-V3-V4* seem related to the ability to deal with spatial and visual relations or, more precisely, to the ability of an individual to visualize and manipulate objects in space. These capacities surely play also a relevant role in determining one's capacity to complete a given series. However, the results of the $23^{th}$ test is necessarily also hugely connected to some problem-solving and logical capabilities, as it requires some imagery capacity, mental visualization skills and part–whole relationship skills.

If we also look at the excluded variables - *V2*, *V20*, *V22* - all seems to be in some sense related to the already identified visual, spatial and logical spheres. This give further support to our interpretation. 

Consequently, we can call this factor "visual,spatial and logic intelligence".

The group of variables associated to the third factor is:
```{r}
group3 <- var.meaning %>% dplyr::filter(var %in% group3) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max), col == 3)$load.max)
group3
```
Even for this factor there is one variable that we do not consider significant, *V21* - actually, we have rounded up the loading of *V24*. So the variables to be considered are:

```{r}
restricted3 <- group3 %>% dplyr::filter(load.max > 0.49)
restricted3
```
Looking at this group - and also taking into account the fact that the variable *V10* representing an addition test load very high on the considered third factor - we may interpret this factor as an "arithmetic intelligence".

To be more precise, we have searched some explanations about the tests performed by Holzinger and Swineford - who collected the data - in order to be able to give a better interpretation of the results. It turned out that both the addition and the counting dots tests were two speeded tests. This fits well with our previous interpretation, since it seems reasonable to believe that the students who solves computation quicker are also the ones with more aptitude toward mathematical reasoning. 

For what concerns the excluded variable *V21*, it corresponds to a numerical-puzzle test and therefore fit quite well in the group associated with an "arithmetic intelligence" factor.


The group of variables associated to the fourth factor is:
```{r}
group4 <- var.meaning %>% dplyr::filter(var %in% group4) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max), col == 4)$load.max)
group4
```
We remove the three variables whose loadings are not enough significant:

```{r}
restricted4 <- group4 %>% dplyr::filter(load.max > 0.5)
restricted4
```

All the variables that load high on the fourth factor are related to the spheres of recognition and associative memory. Thus this factor may be simply called "memory/recognition".

Once again also the excluded variables may be well explained by the "memory/recognition" fourth rotated factor.

Finally, the group of variables associated to the fifth factor is:
```{r}
var.meaning %>% dplyr::filter(var %in% group5) %>% dplyr::mutate(load.max = dplyr::filter(as.data.frame(pos.max), col == 5)$load.max)
```
This group is made of by a unique variable, *V13*, corresponding to a test consisting of a speeded discrimination of straight and curved uppercase letters - we have found the fact that the test were speeded in the description of the data, as already mentioned above.
The factor may be called "speed letter recognition", but is actually of low relevance.

We can now deal with the variables for with there are some doubts in the classification.

In particular, the variables for which we have identified some issues are the following:
```{r}
doubt.var <- rbind(group2 %>% dplyr::filter(load.max < 0.5), group3 %>% dplyr::filter(load.max < 0.5), group4 %>% dplyr::filter(load.max < 0.5))
doubt.var <- round(psych.5fa.ml$loadings[which(var.meaning$var%in%doubt.var$var),],3)
doubt.var <- cbind(doubt.var, dplyr::filter(var.meaning, var %in% row.names(doubt.var)))
doubt.var <- dplyr::select(doubt.var, -var)
doubt.var
```
As we can see, *V2* may properly be associated with the second group, indeed its second factor loading is much greater than all the others.
For the same reason the variables *V18*, *V19*, *V20* and *V24* may be reasonably added to group 4, 4, 2, 3.
So, with the only exceptions of variables *V11* and *V22*, our previous identification of the groups seems to be quite satisfactory, especially if we consider the fact that the proportion cumulative variance explained by the 5 considered common factors is only equal to the 50% of the total variability of the data.

For what concerns the variable *V11*, we can notice that its three highest loadings - corresponding to factors 3, 4 and 5 - are all very similar. This may be reasonable, indeed to score high in a code test it in necessary not only to have good memory and good "matching" capacities (factor 4), but also some mathematical-problem solving skills (factor 3) and the ability to recognize quickly the elements to code (factor 5).

Similarly, for variable *V22* we can't ignore the loading associated to factor 1. Also in this case, we may interpret this result by saying that "problem reasoning" abilities are quite trasversal skills and are surely also related to the verbal sphere.


## Make a scatterplot of the first two factor scores for the m = 5 solution obtained by the regression method. Is their correlation equal to zero? Should we expect so? Comment.

First of all we make some theoretical observations on the factor scores.

The factor scores are the estimated values of the underlying common factors. In particular, they are estimates of the unobserved vector $f_i = (f_{i1}, \dots, f_{im})$ - in place of which we have only observed the variables realisations $x_i = (x_{i1}, \dots, x_{im})$. The estimation, however, is not straightforward, as the total number of unbserved quantities - given not only by the $f_i$, but also by the error terms $\epsilon_i$ - outnumbers the observed $x_i$. 

One of the most used approaches advanced to overcome this problem is the *regression method*. The idea is the following: consider the baseline equation of the factor model
$$
  X - \mu = \mathbb{L}F + \epsilon
$$
where we suppose that both the factors and the errors are jointly normally distributed and that both the common factors and the errors are uncorrelaed within each others. Under these assumption we know that the conditional distribution of $F|X$ is again gaussian, with conditional mean given by
$$
 \mathbb{L}^T\mathbb{\Sigma}^{-1}(X-\mu) = \mathbb{L}^T(\mathbb{LL}^T -\mathbb{\Psi})^{-1}(X-\mu)
$$
Given so, a natural estimate for $f_i$ is simply the corresponding estimate of this conditional mean:
$$
\hat{f_i} = \hat{\mathbb{L}}^T\mathbb{S}^{-1}(x_i-\bar{x})
$$
where we use $\mathbb{S}$ in place of its estimation in order to try to reduce the effects of possible mistakes in determining the number of factors.

Given so, since we assume that the common factors are uncorrelated - $cov(F_i, F_j) = 0, \quad \forall i\neq j$ - we will get - or at least we would like to get - uncorrelated factor scores.

We therefore expect that the first and second factor scores are essentially uncorrelated.

To test our hypothesis we make a scatterplot of the first two factor scores:

```{r}
psych.5fa.ml <- factanal(x=Grant_White, factors = m[1], rotation = "varimax", scores = "regression")
```

```{r}
ggplot(as.data.frame(psych.5fa.ml$scores), aes(x = Factor1, y = Factor2)) + 
  geom_point()
```
As expected, no clear pattern arises from the scatter plot, suggesting that the first two factor scores are almost not correlated within each others. Moreover, a numerical computation of the correlation returns:
```{r}
cor(psych.5fa.ml$scores[,1], psych.5fa.ml$scores[,2])
```
A value that is indeed small.

Finally, a further insightful thing to verify is whether the assumption of bivariate normality actually holds: in particular, by the theory we expect that the first and second factors are jointly normally distributed with mean zero and diagonal covariance matrix. This is translated in a plot in which the points are centered around the origin and the elliptic contour plots have a nice circular shape.

Together with the assumption of normality, it is important to study the presence of bivariate outliers. Indeed, outliers can influence the strength and direction of the correlation. The reason why is that outliers can create a distorted picture of the relationship between the two variables. Thus, if an outlier is included in a dataset, it can pull the correlation coefficient in one direction or another, making it appear stronger or weaker than it actually is, or even falsify the normality assumption.

In our situation, the correlation coefficient is already very low, therefore we do not expect to find bivariate outliers that are extreme observations for both observations. For the same reason, we also expect to find circular contour plots.
```{r}
a <- (n-0.5)/n
label = rep("", n)
p <- ggplot(as.data.frame(psych.5fa.ml$scores), aes(x = Factor1, y = Factor2)) + geom_point() + stat_ellipse(level = a)
# before plotting we determine the potential bivariate outliers.
# Extract components
build <- ggplot_build(p)$data
points <- build[[1]]
ell <- build[[2]]
# Find which points are inside the ellipse, and add this to the data
dat <- data.frame(
  points[1:2], 
  in.ell = as.logical(point.in.polygon(points$x, points$y, ell$x, ell$y))
)
means.hat <- round(colMeans(dat[,-3]),3)
label[!dat$in.ell] = as.character(which(!dat$in.ell))
ggplot(dat, aes(x = x, y = y)) + geom_point(aes(col = in.ell), show.legend = F) + geom_text(mapping = aes(label = label),
            hjust = 1.0, nudge_y = -0.15, nudge_x = 0.05, size = 5, parse = T) +
stat_ellipse(level = 0.95, color = "deepskyblue4", linetype = 2) + stat_ellipse(level = a, color = "darkorange") + scale_color_manual(values = c("red", "black")) #+ geom_point(data = as.data.frame(means.hat), mapping = aes(x=x, y=y), color = "darkorange", size = 3)
```
As expected, the desired assumption of normality appears to hold.


##

```{r}
Pasteur <- psych %>% dplyr::filter(group == "PASTEUR") %>% dplyr::select(-c(Case, Sex, Age, group))
```
```{r}
psych.5fa.ml <- factanal(x=Pasteur, factors = m[1], rotation = "varimax")
psych.5fa.ml$loadings
```

## Exercise 2
```{r}
#setwd("c:/Users/lucia/OneDrive/Desktop/Documenti/Stochastics and Data Science/Multivariate Statistical Analysis/PS2/problemset_2/problemset_2")
#setwd("C:/Users/valer/Desktop/dati_lab_R")
pendigits<-read.table("data_pendigits.txt", sep=",",head=F)
names(pendigits)<-c(paste0(rep(c("x","y"),8),rep(1:8,each=2)),"digit")
dim(pendigits)
head(pendigits)
```

```{r}
lookup<-c("darkgreen", "brown", "lightblue", "magenta", "purple",
"blue", "red", "lightgreen", "orange", "cyan")
names(lookup)<-as.character(0:9)
digit.col<-lookup[as.character(pendigits$digit)]
```

```{r}
n <- nrow(pendigits)
p <- ncol(pendigits)
k <- length(unique(pendigits$digit)); k
```
#### Use linear discriminant analysis (LDA). Display the first two LD variables in a scatterplot, color coding the observations according to variable digit.col below. How well do the discriminate the 10 digits? Refer also to theory. 

First of all, as requested, we proceed by implementing the linear discriminant analysis:
```{r}
lda.fit <- MASS::lda(digit ~ ., data = pendigits)
lda.fit
```

```{r}
lda.fit$scaling
lda.pred <- predict(lda.fit)
```

```{r}
lda.pred$x[1:5,]
lda.pred$x <- as.data.frame(lda.pred$x)
means.hat <- aggregate(lda.pred$x,by=list(pendigits$digit),FUN=mean)[,-1]
```
We can now display in the below scatterplot the first two LD variables:
```{r}
p <- ggplot(as.data.frame(lda.pred$x), mapping = aes(x = LD1, y = LD2)) + 
  geom_point(aes(colour = factor(pendigits$digit)), alpha = 0.7, show.legend = T) +
  stat_ellipse(aes(colour = factor(pendigits$digit))) +
  scale_color_manual(values = c("darkgreen",  "brown", "lightblue",  "magenta", "purple", "blue", "red", "lightgreen", "orange", "cyan")) +
  labs(colour = "digit") + 
  geom_point(as.data.frame(means.hat), mapping = aes(x = LD1, y = LD2, colour = factor(pendigits$digit)), 
             shape = 21, colour = "black", fill = lookup, size = 3.5, stroke = 1.0)
p
```
In the plot we can appreciate that the digit *4* forms a clear and compact cloud at the top and also *2* at the left. Meanwhile for the other digits we have some issues: some clouds have some values a bit too much far away from the centroids (the points with the black boundaries), the worst case is the digit *5*. Also the digit *0* and *8* show this problem. In general all the other digits do not form a compact distribution around their centroids. \\
We can start making some speculations: the digit *4* and *2* have a clearer distributions than the others, because their shape is more distinguishable and unique. Instead, for example the digit *5* could be confused with *9* due to their similar conformations; same for *8* and *0*. Further analysis will follow in next points. 
REFER ALSO TO THEORY???

#### Compute the confusion matrix on the training data. What are the groups more difficult to discriminate from the others? Comment in view of the answer to point 1.

In order to have a better understanding of how well the first two LD variables discriminate the ten digits, we compute the confusion matrix on the training data:
```{r}
conf.mat<-table(predicted=lda.pred$class,true=pendigits$digit)
addmargins(conf.mat)
# n
```
In the confusion matrix we can see the same patterns of the scatterplot: the digit *4* is well discriminated, it has 6 misclassifications and 1116 correct classifications. It is interesting to note that, as previously speculated, the digit *5* has been wrongly predicted as *9* 268 times and the digit *8* and the digit *0* as *8* 77 times. We also remark that the digit *5* is the most difficult to predict with a total of $3+58+9+268=338$ misclassifications, as suggested by the scatterplot.

####Use the leave-one-out cross validation (CV). Compute the confusion matrix and the corresponding CV errors. Is it larger than the trsining error? Why so?

As requested, we proceed with the leave-one-out cross validations:
```{r}
lda.fitCV<-lda(digit~.,data=pendigits, CV=TRUE)
names(lda.fit)
names(lda.fitCV)
names(lda.pred)
```
Now we are able to compute the corresponding confusion matrix and error:
```{r}
conf.mat<-table(predicted=lda.fitCV$class,true=pendigits$digit)
addmargins(conf.mat)
CV.err<-1-mean(lda.fitCV$class==pendigits$digit)
CV.err
(2+1)/n
```
We compute also the training error to make a comparison:
```{r}
training.err<-1-mean(lda.pred$class==pendigits$digit)
training.err
(2+1)/n
```
As we can see, the CV error is around 0.124 and the training error around 0.122, so we have ha difference in favor of the training error of 0.002.\\
WHY SO?
```{r}
groupCV<-rep(1:44, each=250)
groupCV<-groupCV[1:length(pendigits$digit)]
```
